---
title: "T-Tests, Permutation Tests, Bootstrapping and Downsampling"
author: "Colin Quirk"
date: "2/13/2020"
output: html_document
---

```{r setup, message=FALSE}
library(tidyverse)

knitr::opts_chunk$set(message = FALSE)

theme_set(theme_minimal())

set.seed(42)
options(scipen = 10) # Reading scientific notation is hard
```

# T-tests and Permutation Tests

We have seen a bit of comparing two groups in R already, but here we will dive into the topic fully. First we will gain an intuition for what is happening when we are comparing groups and generating p-values by looking at how to do a permutation test. Then we will look at how to do a t.test in R.

To start, let's generate some data. Let's imagine we are running an experiment that results in an average number of words remembered from a list for each of 30 subjects (per group). One group will be the control (group1) which is capable of remembering 10 words on average and the other group will be given some helpful instructions about what they are remembering before the experiment, which will allow them to remember 12 words on average. We will generate the data from the normal distribution.

```{r}
n = 30
group1 = rnorm(n, 10, 1.75)
group2 = rnorm(n, 12, 1.75)

# Creating a data frame for easier operations
data = data.frame(group1 = group1, group2 = group2) %>% 
  pivot_longer(c(group1, group2), names_to = "group")

head(data)
```

Here is what this data looks like when plotted.

```{r}
ggplot(data, aes(x = value, fill = group)) +
  geom_histogram(bins = 10, alpha = 0.75, position = "identity")  # position = "identity" overlaps histograms

ggplot(data, aes(x = group, y = value)) +
  stat_summary() +
  coord_flip()
```

There seems to be a clear difference between groups, but how can we be sure? We need to perform a hypothesis test. To do this, we will create 2 hypotheses, a null (the group effect is equal to 0) and an alternative (the group effect is not equal to 0). All of our conclusions will be based around the null (i.e. we will either reject it or fail to reject it).

What would we expect our data to look like in a universe where the null hypothesis is true? Of course it is possible that due to variability in the data we don't exactly observe a difference of 0 between groups. How can we simulate a group difference of 0 while still keeping the correct amount of variability? The easiest way is to keep our data as is while randomizing the `group` column. After randomization, the `group` column can't possibly contain useful information about the number of words remembered. If we then calculate the "group" difference we know that the value we obtain will be simply due to the natural variability in the data!

Let's create a function that returns a single difference that was generated by chance.

```{r}
get_random_group_diff = function(data) {
  diff_result = data %>% 
    # the '.' here is fancy dplyr syntax that allows you to access the entire dataframe
    # it is necessary here because we want to shuffle only one of the columns
    # in contrast, sample_frac shuffles the entire data frame,
    # which wouldn't change the mean at all
    mutate(group = sample(.$group)) %>% 
    group_by(group) %>% 
    summarise(value = mean(value))
  
  # The syntax here is just saying give me the mean of the
  # value column in the rows where group == "group1"
  # to see each piece, try
  # iris[iris$Species == "setosa",]
  # iris[iris$Species == "setosa",]$Petal.Length
  # mean(iris[iris$Species == "setosa",]$Petal.Length)
  group1_mean = mean(diff_result[diff_result$group == "group1",]$value)
  group2_mean = mean(diff_result[diff_result$group == "group2",]$value)
  
  group2_mean - group1_mean
}

get_random_group_diff(data)
```

Notice that our random difference is not exactly 0, but it is much smaller than the true difference of 2 we used when we created the real data. Now that the groups have been randomized, the above difference must be due to chance. Now if we do this many times we can see what a distribution of values from our function looks like (i.e. the null distribution).

```{r}
null_values = replicate(5000, get_random_group_diff(data))

head(null_values)
```

We want a pretty big number of values here because we are particularly interested in the tails of the distribution (we will be calculating our p-value using these tails). 1000 is good enough to get a sense of what is going on, but if it doesn't take too long you may want to go up to 10000.

Let's take a look at what our distribution looks like.

```{r}
p = ggplot(data.frame(null_values), aes(x = null_values)) +
  geom_histogram()

p
```

As expected, our distribution is nicely centered on 0, but with some spread due to chance. Let's think back to our goal for a second, why were we interested in this again? Remember that this distribution represents values we might expect to see if our null hypothesis is true. So, what difference did we actually observe?

```{r}
(group1_mean = mean(group1))
(group2_mean = mean(group2))

(obs_diff = group2_mean - group1_mean)
```

1.6 seems to be high compared to our null distribution, but let's plot it to see.

```{r}
# geom_vline will give you a vertical line at a specific point
p + geom_vline(xintercept = obs_diff, color='firebrick2', size=2)
```

Take a moment to digest what this plot is showing. Our observed difference is much greater than most of the differences that we would expect to see from chance. It seems like we have a real effect here. But how can we turn this into a p-value? Remember that a p-value tells us the probability that we would have observed our result by chance under the null. So we can simply count up the number of null values that are greater than our observed difference.

```{r}
sum(null_values > obs_diff)
```

We have observed 6 out of 5000 samples that were greater than our observed difference. Remember a p-value is the probability of our result under the null, so we need to divide by the number of samples we collected from the null to get our p-value.

```{r}
6/5000
```

So, the p-value we end up with is 0.0012, which is of course less than 0.05. Depending on our design, it may be appropriate to do a two-tailed test (this should normally be your default). So instead of checking for greater null values, we would check generally for "more extreme" values. The easiest way to do this is to use the `abs` function to take the absolute value of the null_values like so.

```{r}
sum(abs(null_values) > obs_diff)
```

```{r}
13 / 5000
```

So, for our two-tailed test we end up with a p-value of 0.0026 which is significant. Now let's check this against a t-test to see if we end up with a similar result.

```{r}
t.test(group1, group2)
```

So our permutation test resulted in a p-value of 0.0026 whereas the t-test gave us 0.0023. Not bad! `t.test` gives us a two-tailed test by default but read the documentation with `?t.test` to see how to do a one-tailed test, test a difference other than 0, do a paired t-test and more.

Why are the results so similar? They are basically doing the same thing. For a t-test, we assume that our test statistic (i.e. t-value) comes from the t-distribution with df = n - 2. Here is a plot of a t-distribution with df = 58 with our observed t value (3.19) plotted.

```{r}
x = seq(-4, 4, length.out = 1000)
y = dt(x, 58)
t_data = data.frame(x, y)

ggplot(t_data, aes(x = x, y = y)) +
  geom_path() +
  geom_vline(xintercept = 3.19)
```

In the t-test we look at the area under the curve that is more extreme than our observed t-value (instead of counting up simulations), but the principle is the same.

Given how easy the t-test seems to be, it might seem like it was a waste of time to do all that permutation test work. However, one benefit of permutations tests is that you don't need to worry about the assumptions related to t-tests. If you think you have assumption issues, consider using a permutation test.

# T-tests from Scratch

First we will conduct an independent samples t-test from scratch. We already calculated the group means, but we will also need the group sds.

```{r}
group1_mean
group2_mean
(group1_sd = sd(group1))
(group2_sd = sd(group2))
```

Using the formula from the lecture slides, we can calculate our T value. In the homework, it may be easier to work directly with vectors as I am doing here. To get a vector from a data frame, use the `$` syntax, e.g. `iris$Species`

```{r}
standard_error = sqrt((group1_sd^2/length(group1)) + (group2_sd^2/length(group2)))
(t = (group1_mean - group2_mean) / standard_error)
# Note the R performs a degrees of freedom correction by default
(degrees_freedom = length(group1) + length(group2) - 2)
```

As expected, our t value matches our call to `t.test`. To get a p-value, just use the `pt()` function. Make sure to use the correct value for lower.tail (this will depend on whether you have a positive or negative t-value which will depend on the order of your mean subtraction). We multiply by 2 for a two-tailed test.

```{r}
pt(t, df = degrees_freedom) * 2
```

To do a paired sample t-test, we take the difference of the 2 groups we are comparing.

```{r}
differences = group1 - group2

head(differences)
```
 
 Here we use the formula for a one sample t-test.
 
```{r}
(t = mean(differences) / (sd(differences) / sqrt(length(differences))))
(degrees_freedom = length(differences) - 1)
pt(t, df = degrees_freedom) * 2
```
 
 Let's compare this to the built in `t.test` function.
 
```{r}
t.test(group1, group2, paired=TRUE)
```
 
Looks like we did everything correctly.

# Bootstrapping Confidence Intervals

Bootstrapping is very useful for confidence intervals, especially if your normal confidence intervals extend to impossible values.

As an example, let's simulate some data from students taking a math test (with a maximum score of 100%) across two different age groups.

```{r}
three = round(rnorm(8, 50, 10))

five = round(rnorm(8, 50, 10))
# Most of the five year olds have learned a concept that
# allows them to get a perfect score.
five[1:7] = 100

data = data.frame(three = three, five = five) %>% 
  pivot_longer(c(three, five), names_to = 'age') %>% 
  mutate(id = 1:n()) %>% 
  select(id, everything())  # just ordering the columns so id is first

head(data)
```

Let's first plot with normal confidence intervals.

```{r}
ggplot(data, aes(x = age, y = value)) +
  geom_jitter(height = 0, width = 0.2) + # Only jitter along the x axis
  stat_summary(fun.data = mean_cl_normal, color = 'blue')
```

As you can see, the "artificial" limit at 100 is not captured, leading to a confidence interval that doesn't make any sense.

ggplot makes plotting bootstrapped confidence intervals easy.

```{r}
ggplot(data, aes(x = age, y = value)) +
  geom_jitter(height = 0, width = 0.2) + # Only jitter along the x axis
  stat_summary(fun.data = mean_cl_boot, color = 'firebrick2')  # mean_cl_boot does all the work for you
```

As you can see, the bootstrapped confidence interval accounts for the limit in the data.

Let's look into how to calculate this ourselves. We can get a bootstrapped sample with the `sample_frac` function.

```{r}
print(data, n=Inf)  # forcing R to print all the rows

data %>% 
  sample_frac(1, replace=TRUE) %>% 
  print(n = Inf)
```

How to use `sample_frac` depends on exactly what you want to do with your bootstrapped samples, but here is an example of how to get the confidence intervals.

First, we will create a function that returns a mean for each of our groups.

```{r}
get_boot_mean = function(data) {
  boot_means = data %>% 
    group_by(age) %>%  # Ensures we get exactly 8 samples per group
    sample_frac(1, replace = TRUE) %>% 
    summarise(value = mean(value))
  
  boot_means$value  # return just the values so replicate can return a matrix
}
```

Now we can use our function with replicate.

```{r}
boot_means = replicate(1000, get_boot_mean(data))

boot_means[,1:5]

(rowMeans(boot_means))  # Sample means
quantile(boot_means[1,], probs = c(0.025, 0.975))  # 95% CI for age 5
quantile(boot_means[2,], probs = c(0.025, 0.975))  # 95% CI for age 3
```

# Downsampling

In our lab, we use downsampling a lot for understanding the interaction between power and number of trials. In general, we feel that number of trials is an under explored aspect of power. More trials will reduce the error in your estimates, which will reduce your sd's but it is unclear how to determine how many trials you need in an experiement. Usually researchers depend on rules of thumb or use numbers from previous work. Downsampling is a great alternative if you have a large dataset to use.

See here for an example of a project in this vein: https://osf.io/2a4uy/

Here we will simulate an oddball task with a very large number of trials and n = 10.

```{r}
# rbinom generates binary data with a given probability of getting 1
standard = c(replicate(10, rbinom(1000, 1, 0.85)))  # c turns our matrix into a 1-d array
oddball = c(replicate(10, rbinom(1000, 1, 0.75)))

data = data.frame(subject = rep(1:10, each=1000), standard = standard, oddball = oddball) %>% 
  pivot_longer(c(standard, oddball), names_to = "condition")

head(data)

mean_per_subject = data %>% 
  group_by(subject, condition) %>% 
  summarise(value = mean(value))

head(mean_per_subject)
```

```{r}
ggplot(mean_per_subject, aes(x = condition, y = value)) +
  geom_jitter(height=0, width = 0.2) +
  stat_summary(color = 'firebrick2')
```

10 subjects isn't very many but with such a large number of trials the result clearly comes out. But could we have still had good power with a smaller number of trials?

Let's create a function that gets a mean for each subject for each condition given a number of randomly selected trials and returns whether a t-test returns a significant result.

```{r}
get_downsample_means = function(data, n){
  downsample_means = data %>% 
    group_by(subject, condition) %>%
    sample_n(n) %>% 
    summarise(value = mean(value))
  
  downsample_means
  
  res = t.test(downsample_means[downsample_means$condition == 'standard',]$value,
               downsample_means[downsample_means$condition == 'oddball',]$value)
  
  res$p.value < 0.05
}
```

For the sake of simplicity, let's just look at the case where we get only 100 trials instead of 1000 (through you can imagine repeating this process over many different values for the number of trials).

```{r}
sig_results_100 = replicate(1000, get_downsample_means(data, 100))

mean(sig_results_100)
```

Even with only 100 trials, we are still easily detecting the large difference the majority of the time. Let's look at 10 trials for a comparision.

```{r}
sig_results_10 = replicate(1000, get_downsample_means(data, 10))

mean(sig_results_10)
```

While 100 trials was still good enough, it's clear that 10 trials is not.

